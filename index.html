<!DOCTYPE html>
<html>
  <head>
    <title>ICL-VC Demo Page</title>
    <link
      href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css"
      rel="stylesheet"
    />
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.6.0/jquery.min.js"></script>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML" async></script>
    <script src="load_table.js" defer></script>
    <style>
      @import url(https://fonts.googleapis.com/css?family=Open+Sans);
      body {
        font-family: 'Open-Sans', sans-serif;
        font-weight: 300;
        background-color: #fff;
      }
      td {
        vertical-align: middle;
        text-align: justify;
        width: 0vw;
        min-width: 250px;
      }
      audio {
        width: 14vw;
        min-width: 100px;
      }
      .content {
        width: 69vw;
        padding: 25px 50px;
        margin: 25px auto;
        background-color: white;
        box-shadow: 0px 0px 10px #999;
        border-radius: 15px;
        font-family: "Google Sans";
      }
      .contentblock {
        width: 950px;
        margin: 0 auto;
        padding: 0;
        border-spacing: 25px 0;
      }
      .contentblock td {
        background-color: #fff;
        padding: 25px 50px;
        vertical-align: top;
        box-shadow: 0px 0px 10px #999;
        border-radius: 15px;
      }
      a, a:visited {
        color: #224b8d;
        font-weight: 300;
      }
      #authors {
        text-align: center;
        margin-bottom: 20px;
        font-size: 20px;
      }
      #conference {
        text-align: center;
        margin-bottom: 20px;
        font-style: italic;
      }
      #authors a {
        margin: 0 10px;
      }
      h1 {
        text-align: center;
        font-size: 35px;
        font-weight: 300;
      }
      h2 {
        font-size: 30px;
        font-weight: 300;
      }
      h3 {
        font-size: 25px;
        font-weight: 300;
      }
      h4 {
        font-size: 20px;
        font-weight: 300;
      }
      code {
        display: block;
        padding: 10px;
        margin: 10px 10px;
      }
      p {
        line-height: 25px;
        text-align: justify;
      }
      p code {
        display: inline;
        padding: 0;
        margin: 0;
      }
      #teasers {
        margin: 0 auto;
      }
      #teasers td {
        margin: 0 auto;
        text-align: center;
        padding: 5px;
      }
      #teasers img {
        width: 250px;
      }
      #results img {
        width: 133px;
      }
      #seeintodark {
        margin: 0 auto;
      }
      #sift {
        margin: 0 auto;
      }
      #sift img {
        width: 250px;
      }
      .downloadpaper {
        padding-left: 20px;
        float: right;
        text-align: center;
      }
      .downloadpaper a {
        font-weight: bold;
        text-align: center;
      }
      .teaser-img {
        width: 80%;
        display: block;
        margin-left: auto;
        margin-right: auto;
      }
      .teaser-gif {
        display: block;
        margin-left: auto;
        margin-right: auto;
      }
      .summary-img {
        width: 100%;
        display: block;
        margin-left: auto;
        margin-right: auto;
      }
      .video-iframe {
        width: 1000;
        height: 800;
        margin: auto;
        display: block;
      }
      .container {
        display: flex;
        align-items: center;
        justify-content: center
      }
      .image {
        flex-basis: 40%
      }
      .text {
        font-size: 20px;
        padding-left: 20px;
      }
      .center {
        margin-left: auto;
        margin-right: auto;
      }
      .boxshadow {
        border: 1px solid;
        padding: 10px;
        box-shadow: 2px 2px 5px #888888;
      }
      .spacertr {
        height: 8px;
      }
      .spacertd {
        width: 40px;
      }
      .heading-size {
      font-size: 33px;
      }
    </style>
  </head>
  <body>
    <div class="content">
      <div class="text-center">
        <h1 class="heading-size"><strong>Disentangling the Prosody and Semantic Information with Pre-trained Model for In-Context Learning based Zero-Shot Voice Conversion</strong></h1>
        <h5></h5>
      </div>
        <h2 style="text-align:center;">Abstract</h2>
        <div style="text-align: justify;">
           Voice conversion (VC) aims to alter the speaker's timbre while preserving the speech content. In some previous work, outputs from self-supervised pre-trained models are tokenized to obtain semantic tokens, which help disentangle speech content information. Recently, in-context learning (ICL) has gained popularity for its ability to model specific information, like timbre, by simply conditioning on context input in text-to-speech (TTS) systems. This paper introduces an ICL method with semantic tokens for VC, employing a mask and reconstruction training strategy based on a flow-matching generative model. Our experiments on the LibriTTS dataset demonstrate that the ICL method with semantic tokens enhances speaker similarity in VC systems. Additionally, we find that k-means is a versatile tokenization method applicable to various pre-trained models. However, the ICL-based VC system faces challenges in preserving the prosody of the source speech. To address this, we propose extracting prosody embeddings from a pre-trained emotion recognition model and incorporating them into our ICL-based VC system. The integration of prosody embeddings significantly improves our system's ability to maintain the prosody of the source speech, as evidenced by results on the Emotional Speech Database.
        </div>
      </p>
    </div>
    <div class="content">
      <h2 id="model-overview" style="text-align: center;">Architecture Overview</h2>
      <body>
      <p style="text-align: center;">
        <img src="picture/expressive_vc.png" style="width: 60vw; margin-left: 0vw;">
      </p>
        <div style="text-align: justify;">
          The overview of the ICL-VC system. The content information from speech is disentangled using the pre-trained HuBERT or Wav2Vec model, and the prosody information is disentangled using the pre-trained Emotion2Vec model. A mask and reconstruction strategy is used in the training processing to enable the system to have the in-context-learning ability.

        </div>
      </body>
    </div>


    <div class="content">
      <h2 style="text-align: center;">Zero-shot Voice Conversion Samples</h2>
      <div style="text-align: justify;">
      <body>
        <div style="text-align: justify;">
          This section evaluates the systems' zero-shot voice conversion capability. It converts the timbre of the source speech to match the timbre of the reference speech while preserving the content of the source speech. (The source and reference speeches are selected from the LibriTTS test-clean set.)
        </div>
      </body>
      <HR>
    </div>
    <div style="margin-top: 0vh; text-align: center;">
      <div class="table-responsive pt-3">
        <ul class="pagination justify-content-center">
          <li class="page-item active">
            <a id="basic_vc-1" class="page-link" href="#">1</a>
          </li>
          <li class="page-item">
            <a id="basic_vc-2" class="page-link" href="#">2</a>
          </li>
        </ul>
        <table
          class="table pt-2"
          id="basic_vc"
        >
          <thead>
            <tr>
              <!-- <th>Text</th> -->
              <th style="text-align: center;">Reference Speech</th>
              <th style="text-align: center">Source Speech</th>
              <th style="text-align: center">YourTTS</th>
              <th style="text-align: center">RefXVC</th>
              <th style="text-align: center">ICL-VC</th>
            </tr>
          </thead>
          <tbody></tbody>
        </table>
      </div>
    </div>
    </div>

    <div class="content">
      <h2 style="text-align: center;">Prosody Preserving Evaluation Samples</h2>
      <div style="text-align: justify;">
      <body>
        <div style="text-align: justify;">
          This section evaluates the system's ability to preserve the prosody of the source speech during the voice conversion process. (The reference speeches are selected from the LibriTTS test-clean set and the source speeches are selected from the English subset of Emotional Speech Database.)
        </div>
      </body>
      <HR>
    </div>
    <div style="margin-top: 0vh; text-align: center;">
      <div class="table-responsive pt-3">
        <ul class="pagination justify-content-center">
          <li class="page-item active">
            <a id="prosody_vc-1" class="page-link" href="#">1</a>
          </li>
          <li class="page-item">
            <a id="prosody_vc-2" class="page-link" href="#">2</a>
          </li>
          <li class="page-item">
            <a id="prosody_vc-3" class="page-link" href="#">3</a>
          </li>
        </ul>
        <table
          class="table pt-2"
          id="prosody_vc"
        >
          <thead>
            <tr>
              <!-- <th>Text</th> -->
              <th style="text-align: center;">Reference Speech</th>
              <th style="text-align: center">Source Speech</th>
              <th style="text-align: center">YourTTS</th>
              <th style="text-align: center">RefXVC</th>
              <th style="text-align: center">ICL-VC</th>
              <th style="text-align: center">ICL-VC (Pitch & Energy)</th>
              <th style="text-align: center">ICL-VC (Emotion Emb)</th>
            </tr>
          </thead>
          <tbody></tbody>
        </table>
      </div>
    </div>
    </div>

    <!-- <div class="content">
      <h2 id="ethics-statement" style="text-align: center;">Ethics Statement</h2>
      <body>
        <div style="text-align: justify;">
          Since ICL-VC could synthesize speech that transforms speaker identity, it may carry potential risks in misuse of the model, such as spoofing voice identification or impersonating a specilic speaker. We conducted the experiments under the asumption that the user agree to be the taret speaker in speech synthesis. If the model is generalized to unseen speakers in the real world, it should include a protocol to ensure that the speaker approves the use of their voice and a synthesized speech detection model.
        </div>
      </body>
    </div> -->


    <!--<p style="text-align: center;">-->
    <!--<img src="https://badges.toozhao.com/badges/01HW75WW0HVJR8Q66FJKQ7ZE07/green.svg" />-->
    <!--</p>-->
  </body>

</html>
